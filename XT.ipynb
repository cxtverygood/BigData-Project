{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51e3ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "| true| 3990|\n",
      "|false| 3969|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "# Match all the csv files\n",
    "file_paths = glob.glob('train-*.csv')\n",
    "spark = SparkSession.builder.appName(\"BigData\").getOrCreate()\n",
    "# Read data\n",
    "train_data = pd.concat([pd.read_csv(file) for file in file_paths], ignore_index=True)\n",
    "\n",
    "directing_data = pd.read_json('directing.json')\n",
    "writing_data = pd.read_json('writing.json')\n",
    "# Combine the columns\n",
    "# train_data = pd.merge(train_data, directing_data, left_on='tconst', right_on='movie', how='left')\n",
    "# train_data = pd.merge(train_data, writing_data, left_on='tconst', right_on='movie', how='left')\n",
    "# train_data.drop(['movie_x', 'movie_y'], axis=1, inplace=True)\n",
    "\n",
    "# Check the first few lines\n",
    "train_data.to_csv(\"trained_data.csv\")\n",
    "train_data = spark.read.csv(\"trained_data.csv\", header=True, inferSchema=True)\n",
    "# Select columns to keep, excluding the first two columns\n",
    "columns_to_keep = train_data.columns[2:]\n",
    "\n",
    "# Create a new DataFrame with the desired columns\n",
    "train_data = train_data.select(*[col(column) for column in columns_to_keep])\n",
    "# Show the statistic information\n",
    "train_data.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed8c27",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8432d953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+--------+-----+-----------+\n",
      "|   tconst|        primaryTitle|runtimeMinutes|numVotes|label|ReleaseYear|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+\n",
      "|tt0010600|            The Doll|            66|  1898.0| true|       1919|\n",
      "|tt0011841|       Way Down East|           145|  5376.0| true|       1920|\n",
      "|tt0012494|             Déstiny|            97|  5842.0| true|       1921|\n",
      "|tt0015163|       The Navigator|            59|  9652.0| true|       1924|\n",
      "|tt0016220|The Phantom of th...|            93| 17887.0| true|       1925|\n",
      "|tt0016630|     Báttling Bútlér|            77|  3285.0| true|       1926|\n",
      "|tt0024184|   The Invisible Man|            71| 33562.0| true|       1933|\n",
      "|tt0024216|           King Kong|           100| 83177.0| true|       1933|\n",
      "|tt0030341|   The Lady Vanishes|            96| 50707.0| true|       1938|\n",
      "|tt0032156|The Story of the ...|           143|  3600.0| true|       1939|\n",
      "|tt0033022|         Sáps át Séá|            56|  2766.0| true|       1940|\n",
      "|tt0034862|         Holiday Inn|           100| 14436.0| true|       1942|\n",
      "|tt0036260|        Phantom Lady|            87|  4833.0| true|       1944|\n",
      "|tt0036716|         Cobra Woman|            71|  1101.0|false|       1944|\n",
      "|tt0037913|      Mildréd Piércé|           111| 24727.0| true|       1945|\n",
      "|tt0037939|The Naughty Nineties|            76|  1957.0| true|       1945|\n",
      "|tt0038733|  Stáirwáy tớ Héávén|           104| 21218.0| true|       1946|\n",
      "|tt0039676|     The October Man|            95|  1260.0| true|       1947|\n",
      "|tt0040843|The Street with N...|            91|  2910.0| true|       1948|\n",
      "|tt0042742|          Mister 880|            90|  1519.0| true|       1950|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the year of the movie\n",
    "#def preprocessing\n",
    "train_data = train_data.withColumn(\n",
    "    \"ReleaseYear\",\n",
    "    when(col(\"endYear\") != \"\\\\N\", col(\"endYear\")).otherwise(col(\"startYear\"))\n",
    ")\n",
    "train_data = train_data.drop(\"startYear\", \"endYear\", \"originalTitle\")\n",
    "train_data = train_data.dropna()\n",
    "for column_name in [\"primaryTitle\", \"originalTitle\"]:\n",
    "    train_data = train_data.filter(col(column_name).isNotNull() & (col(column_name).cast(\"string\") == col(column_name)))\n",
    "   \n",
    "train_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdded541",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1771329f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+\n",
      "|   tconst|        primaryTitle|runtimeMinutes|numVotes|label|ReleaseYear|isNewRelease|runtimeBucket|logVotes|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+\n",
      "|tt0010600|            The Doll|          66.0|  1898.0| true|       1919|           0|          0.0|    7.55|\n",
      "|tt0011841|       Way Down East|         145.0|  5376.0| true|       1920|           0|          3.0|    8.59|\n",
      "|tt0012494|             Déstiny|          97.0|  5842.0| true|       1921|           0|          1.0|    8.67|\n",
      "|tt0015163|       The Navigator|          59.0|  9652.0| true|       1924|           0|          0.0|    9.18|\n",
      "|tt0016220|The Phantom of th...|          93.0| 17887.0| true|       1925|           0|          1.0|    9.79|\n",
      "|tt0016630|     Báttling Bútlér|          77.0|  3285.0| true|       1926|           0|          0.0|     8.1|\n",
      "|tt0024184|   The Invisible Man|          71.0| 33562.0| true|       1933|           0|          0.0|   10.42|\n",
      "|tt0024216|           King Kong|         100.0| 83177.0| true|       1933|           0|          2.0|   11.33|\n",
      "|tt0030341|   The Lady Vanishes|          96.0| 50707.0| true|       1938|           0|          1.0|   10.83|\n",
      "|tt0032156|The Story of the ...|         143.0|  3600.0| true|       1939|           0|          3.0|    8.19|\n",
      "|tt0033022|         Sáps át Séá|          56.0|  2766.0| true|       1940|           0|          0.0|    7.93|\n",
      "|tt0034862|         Holiday Inn|         100.0| 14436.0| true|       1942|           0|          2.0|    9.58|\n",
      "|tt0036260|        Phantom Lady|          87.0|  4833.0| true|       1944|           0|          0.0|    8.48|\n",
      "|tt0036716|         Cobra Woman|          71.0|  1101.0|false|       1944|           0|          0.0|     7.0|\n",
      "|tt0037913|      Mildréd Piércé|         111.0| 24727.0| true|       1945|           0|          2.0|   10.12|\n",
      "|tt0037939|The Naughty Nineties|          76.0|  1957.0| true|       1945|           0|          0.0|    7.58|\n",
      "|tt0038733|  Stáirwáy tớ Héávén|         104.0| 21218.0| true|       1946|           0|          2.0|    9.96|\n",
      "|tt0039676|     The October Man|          95.0|  1260.0| true|       1947|           0|          1.0|    7.14|\n",
      "|tt0040843|The Street with N...|          91.0|  2910.0| true|       1948|           0|          1.0|    7.98|\n",
      "|tt0042742|          Mister 880|          90.0|  1519.0| true|       1950|           0|          1.0|    7.33|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, log1p, round\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Get basic information\n",
    "def get_information(data):\n",
    "    # Average value calculation\n",
    "    avg_runtime = data.select(avg(\"runtimeMinutes\")).collect()[0][0]\n",
    "    avg_numVotes = data.select(avg(\"numVotes\")).collect()[0][0]\n",
    "    avg_ReleaseYear = data.select(avg(\"ReleaseYear\")).collect()[0][0]\n",
    "    \n",
    "    \n",
    "    return int(avg_runtime), int(avg_numVotes), int(avg_ReleaseYear)\n",
    "\n",
    "# Print the result\n",
    "avg_runtime, avg_numVotes, avg_ReleaseYear = get_information(train_data)\n",
    "# Create a binary feature to indicate whether it is a new movie\n",
    "train_data = train_data.withColumn(\"isNewRelease\", (train_data[\"releaseYear\"] <= avg_runtime).cast(\"int\"))\n",
    "\n",
    "# Calculate the quantiles\n",
    "def calculate_quantiles(data: DataFrame, column: str, quantiles: list):\n",
    "\n",
    "    # Convert quantiles to summary statistics\n",
    "    statistics = [\"{}%\".format(int(quantile * 100)) for quantile in quantiles]\n",
    "\n",
    "    # Use summary() function to calculate quantiles\n",
    "    summary_df = data.select(column).summary(*statistics)\n",
    "\n",
    "    # Convert summary DataFrame to Pandas DataFrame for easier manipulation\n",
    "    summary_df_pd = summary_df.toPandas()\n",
    "\n",
    "    # Extract quantile values from Pandas DataFrame\n",
    "    quantile_values = {}\n",
    "    for quantile in quantiles:\n",
    "        quantile_index = int(quantile * len(summary_df_pd)) - 1\n",
    "        quantile_value = summary_df_pd.iloc[quantile_index][column]\n",
    "        quantile_values[quantile] = quantile_value\n",
    "\n",
    "    return quantile_values\n",
    "\n",
    "# Calculate quantiles\n",
    "train_data = train_data.withColumn(\"runtimeMinutes\", train_data[\"runtimeMinutes\"].cast(\"float\"))\n",
    "\n",
    "quantiles = [0,0.25, 0.5, 0.75]\n",
    "column = \"runtimeMinutes\"\n",
    "quantile_values = calculate_quantiles(train_data, column, quantiles)\n",
    "\n",
    "# Convert quantile values to integers\n",
    "quantile_values = {k: float(v) for k, v in quantile_values.items()}\n",
    "\n",
    "# Convert quantile values to a list of floats\n",
    "sorted_quantiles = sorted(quantile_values.items(), key=lambda x: x[1], reverse=False)\n",
    "quantile_list = [float(val) for key, val in sorted_quantiles]\n",
    "\n",
    "\n",
    "# # Create a Bucketizer instance\n",
    "splits = quantile_list +[float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"runtimeMinutes\", outputCol=\"runtimeBucket\")\n",
    "\n",
    "# Apply Bucketizer to the data\n",
    "train_data = bucketizer.transform(train_data)\n",
    "\n",
    "\n",
    "train_data = train_data.withColumn(\"logVotes\", log1p(train_data[\"numVotes\"]))\n",
    "train_data = train_data.withColumn(\"logVotes\", round(train_data[\"logVotes\"], 2))\n",
    "train_data.dropna()\n",
    "train_data.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1ae79c",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f2bc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+--------------------+---------------------+--------------------+--------------------+\n",
      "|   tconst|        primaryTitle|runtimeMinutes|numVotes|label|ReleaseYear|isNewRelease|runtimeBucket|logVotes|  primaryTitle_words|primaryTitle_features|  primaryTitle_tfidf|            features|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+--------------------+---------------------+--------------------+--------------------+\n",
      "|tt0010600|            The Doll|          66.0|  1898.0| true|       1919|           0|          0.0|    7.55|         [the, doll]| (1000,[17,685],[1...|(1000,[17,685],[1...|(1006,[17,685,100...|\n",
      "|tt0011841|       Way Down East|         145.0|  5376.0| true|       1920|           0|          3.0|    8.59|   [way, down, east]| (1000,[217,391,57...|(1000,[217,391,57...|(1006,[217,391,57...|\n",
      "|tt0012494|             Déstiny|          97.0|  5842.0| true|       1921|           0|          1.0|    8.67|           [déstiny]|    (1000,[52],[1.0])|(1000,[52],[4.923...|(1006,[52,1000,10...|\n",
      "|tt0015163|       The Navigator|          59.0|  9652.0| true|       1924|           0|          0.0|    9.18|    [the, navigator]| (1000,[17,407],[1...|(1000,[17,407],[1...|(1006,[17,407,100...|\n",
      "|tt0016220|The Phantom of th...|          93.0| 17887.0| true|       1925|           0|          1.0|    9.79|[the, phantom, of...| (1000,[17,495,541...|(1000,[17,495,541...|(1006,[17,495,541...|\n",
      "|tt0016630|     Báttling Bútlér|          77.0|  3285.0| true|       1926|           0|          0.0|     8.1|  [báttling, bútlér]| (1000,[499,804],[...|(1000,[499,804],[...|(1006,[499,804,10...|\n",
      "|tt0024184|   The Invisible Man|          71.0| 33562.0| true|       1933|           0|          0.0|   10.42|[the, invisible, ...| (1000,[17,569,777...|(1000,[17,569,777...|(1006,[17,569,777...|\n",
      "|tt0024216|           King Kong|         100.0| 83177.0| true|       1933|           0|          2.0|   11.33|        [king, kong]| (1000,[494,710],[...|(1000,[494,710],[...|(1006,[494,710,10...|\n",
      "|tt0030341|   The Lady Vanishes|          96.0| 50707.0| true|       1938|           0|          1.0|   10.83|[the, lady, vanis...| (1000,[17,351,939...|(1000,[17,351,939...|(1006,[17,351,939...|\n",
      "|tt0032156|The Story of the ...|         143.0|  3600.0| true|       1939|           0|          3.0|    8.19|[the, story, of, ...| (1000,[1,17,495,8...|(1000,[1,17,495,8...|(1006,[1,17,495,8...|\n",
      "|tt0033022|         Sáps át Séá|          56.0|  2766.0| true|       1940|           0|          0.0|    7.93|     [sáps, át, séá]| (1000,[198,278,82...|(1000,[198,278,82...|(1006,[198,278,82...|\n",
      "|tt0034862|         Holiday Inn|         100.0| 14436.0| true|       1942|           0|          2.0|    9.58|      [holiday, inn]| (1000,[148,703],[...|(1000,[148,703],[...|(1006,[148,703,10...|\n",
      "|tt0036260|        Phantom Lady|          87.0|  4833.0| true|       1944|           0|          0.0|    8.48|     [phantom, lady]| (1000,[351,957],[...|(1000,[351,957],[...|(1006,[351,957,10...|\n",
      "|tt0036716|         Cobra Woman|          71.0|  1101.0|false|       1944|           0|          0.0|     7.0|      [cobra, woman]| (1000,[504,563],[...|(1000,[504,563],[...|(1006,[504,563,10...|\n",
      "|tt0037913|      Mildréd Piércé|         111.0| 24727.0| true|       1945|           0|          2.0|   10.12|   [mildréd, piércé]| (1000,[166,196],[...|(1000,[166,196],[...|(1006,[166,196,10...|\n",
      "|tt0037939|The Naughty Nineties|          76.0|  1957.0| true|       1945|           0|          0.0|    7.58|[the, naughty, ni...| (1000,[6,17,759],...|(1000,[6,17,759],...|(1006,[6,17,759,1...|\n",
      "|tt0038733|  Stáirwáy tớ Héávén|         104.0| 21218.0| true|       1946|           0|          2.0|    9.96|[stáirwáy, tớ, hé...| (1000,[264,615,69...|(1000,[264,615,69...|(1006,[264,615,69...|\n",
      "|tt0039676|     The October Man|          95.0|  1260.0| true|       1947|           0|          1.0|    7.14| [the, october, man]| (1000,[17,458,569...|(1000,[17,458,569...|(1006,[17,458,569...|\n",
      "|tt0040843|The Street with N...|          91.0|  2910.0| true|       1948|           0|          1.0|    7.98|[the, street, wit...| (1000,[15,17,650,...|(1000,[15,17,650,...|(1006,[15,17,650,...|\n",
      "|tt0042742|          Mister 880|          90.0|  1519.0| true|       1950|           0|          1.0|    7.33|       [mister, 880]| (1000,[132,617],[...|(1000,[132,617],[...|(1006,[132,617,10...|\n",
      "+---------+--------------------+--------------+--------+-----+-----------+------------+-------------+--------+--------------------+---------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, VectorAssembler\n",
    "from pyspark.sql.functions import col, concat_ws\n",
    "\n",
    "# Create tokenizers\n",
    "tokenizer = Tokenizer(inputCol=\"primaryTitle\", outputCol=\"primaryTitle_words\")\n",
    "\n",
    "\n",
    "# Tokenize movie titles\n",
    "data_tokenized = tokenizer.transform(train_data)\n",
    "\n",
    "# Use HashingTF to convert words to feature vectors\n",
    "hashingTF = HashingTF(inputCol=\"primaryTitle_words\", outputCol=\"primaryTitle_features\", numFeatures=1000)\n",
    "\n",
    "\n",
    "data_hashed = hashingTF.transform(data_tokenized)\n",
    "\n",
    "\n",
    "# Calculate TF-IDF values\n",
    "idf = IDF(inputCol=\"primaryTitle_features\", outputCol=\"primaryTitle_tfidf\")\n",
    "\n",
    "idfModel = idf.fit(data_hashed)\n",
    "data_tfidf = idfModel.transform(data_hashed)\n",
    "\n",
    "# Cast ReleaseYear column to integer\n",
    "data_tfidf = data_tfidf.withColumn(\"ReleaseYear\", col(\"ReleaseYear\").cast(\"int\"))\n",
    "# Merge other features\n",
    "feature_cols = [\"primaryTitle_tfidf\",\"runtimeMinutes\", \"numVotes\", \"ReleaseYear\", \"isNewRelease\", \"runtimeBucket\", \"logVotes\"]\n",
    "assembler = VectorAssembler(inputCols= feature_cols, outputCol=\"features\")\n",
    "\n",
    "data_final = assembler.transform(data_tfidf)\n",
    "\n",
    "# data_final = data_final.drop(\"primaryTitle_words\")\n",
    "\n",
    "# View the results\n",
    "data_final.show()\n",
    "# data_final.write.csv(\"final.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0275b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'primaryTitle_tfidf' data type: vector\n",
      "Column 'runtimeMinutes' data type: float\n",
      "Column 'numVotes' data type: double\n",
      "Column 'ReleaseYear' data type: int\n",
      "Column 'isNewRelease' data type: int\n",
      "Column 'runtimeBucket' data type: double\n",
      "Column 'logVotes' data type: double\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "for col_name in [\"primaryTitle_tfidf\", \"runtimeMinutes\", \"numVotes\", \"ReleaseYear\", \"isNewRelease\", \"runtimeBucket\", \"logVotes\"]:\n",
    "    print(f\"Column '{col_name}' data type: {data_final.select(col_name).dtypes[0][1]}\")\n",
    "data_final = data_final.drop(\"primaryTitle_features\", \"primaryTitle_tfidf\", \"primaryTitle_words\")\n",
    "# data_final.write.csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758fe46b",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "dffc59f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7853.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 901.0 failed 1 times, most recent failure: Lost task 0.0 in stage 901.0 (TID 643) (host.docker.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000001cec224bb88`: (struct<primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,runtimeMinutes_double_VectorAssembler_406dc38afd14:double,numVotes:double,ReleaseYear_double_VectorAssembler_406dc38afd14:double,isNewRelease_double_VectorAssembler_406dc38afd14:double,runtimeBucket:double,logVotes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000001cec224bb88`: (struct<primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,runtimeMinutes_double_VectorAssembler_406dc38afd14:double,numVotes:double,ReleaseYear_double_VectorAssembler_406dc38afd14:double,isNewRelease_double_VectorAssembler_406dc38afd14:double,runtimeBucket:double,logVotes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, labelCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mfit(training_data)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate model performance using test data\u001b[39;00m\n\u001b[0;32m     18\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtransform(test_data)\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_java(dataset)\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mfit(dataset\u001b[38;5;241m.\u001b[39m_jdf)\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mH:\\Anaconda\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o7853.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 901.0 failed 1 times, most recent failure: Lost task 0.0 in stage 901.0 (TID 643) (host.docker.internal executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000001cec224bb88`: (struct<primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,runtimeMinutes_double_VectorAssembler_406dc38afd14:double,numVotes:double,ReleaseYear_double_VectorAssembler_406dc38afd14:double,isNewRelease_double_VectorAssembler_406dc38afd14:double,runtimeBucket:double,logVotes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (`VectorAssembler$$Lambda/0x000001cec224bb88`: (struct<primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,primaryTitle_tfidf:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>,runtimeMinutes_double_VectorAssembler_406dc38afd14:double,numVotes:double,ReleaseYear_double_VectorAssembler_406dc38afd14:double,isNewRelease_double_VectorAssembler_406dc38afd14:double,runtimeBucket:double,logVotes:double>) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>).\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:198)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: org.apache.spark.SparkException: Encountered null while assembling a row with handleInvalid = \"error\". Consider\nremoving nulls from dataset or using handleInvalid = \"keep\" or \"skip\".\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1(VectorAssembler.scala:291)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.$anonfun$assemble$1$adapted(VectorAssembler.scala:260)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)\r\n\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:260)\r\n\tat org.apache.spark.ml.feature.VectorAssembler.$anonfun$transform$6(VectorAssembler.scala:143)\r\n\t... 20 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "assembler = VectorAssembler(inputCols= feature_cols, outputCol=\"features\")\n",
    "\n",
    "data_final = assembler.transform(data_tfidf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert boolean labels to numerical values, e.g., True -> 1, False -> 0\n",
    "data_final = data_final.withColumn(\"label\", when(col(\"label\") == True, 1).otherwise(0))\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "training_data, test_data = data_final.randomSplit([0.8, 0.2], seed=123)\n",
    "\n",
    "# Define Random Forest Classifier\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# Train the model\n",
    "model = rf.fit(training_data)\n",
    "\n",
    "# Evaluate model performance using test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator.evaluate(predictions)\n",
    "\n",
    "# Print model performance\n",
    "print(\"Area Under ROC (AUC) on test data: {}\".format(auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac4d8a",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceb66cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Benchmark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Do one-hot on non-value type data\n",
    "train_data_encoded = pd.get_dummies(train_data, columns=['tconst', 'primaryTitle', 'originalTitle', 'startYear', 'director','movie_writing', 'writer'])\n",
    "\n",
    "\n",
    "train_data_encoded = train_data_encoded.drop(['movie_train'], axis=1)\n",
    "\n",
    "# Encoding\n",
    "train_data_encoded['label'] = train_data_encoded['label'].astype(int)\n",
    "\n",
    "# Handling missing values\n",
    "train_data_encoded.replace('\\\\N', np.nan, inplace=True)\n",
    "\n",
    "missing_values = train_data_encoded.isnull().sum()\n",
    "\n",
    "\n",
    "for column in missing_values[missing_values > 0].index:\n",
    "    if train_data_encoded[column].dtype == 'float64':\n",
    "        \n",
    "        train_data_encoded[column].fillna(train_data_encoded[column].mean(), inplace=True)\n",
    "    else:\n",
    "       \n",
    "        train_data_encoded[column].fillna(train_data_encoded[column].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Feature extraction\n",
    "\n",
    "# X_train = train_data_encoded.drop(['label', 'tconst_encoded'], axis=1)\n",
    "X_train = train_data_encoded.drop(['label'], axis=1)\n",
    "y_train = train_data_encoded['label']\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test prediction\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Print report\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8a3d2",
   "metadata": {},
   "source": [
    "# Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad20fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv('test_hidden.csv')\n",
    "\n",
    "# Drop unnecessary columns\n",
    "test_data = test_data.drop(['endYear'], axis=1)\n",
    "\n",
    "# Merge directing and writing data\n",
    "test_data = test_data.merge(directing_data, left_on='tconst', right_on='movie', how='left', suffixes=('_train', '_directing'))\n",
    "test_data = test_data.merge(writing_data, left_on='tconst', right_on='movie', how='left', suffixes=('_train', '_writing'))\n",
    "\n",
    "# Check the column names after the merge operation\n",
    "print(test_data.columns)\n",
    "\n",
    "# Feature extraction for the test set\n",
    "X_test = pd.get_dummies(test_data, columns=['tconst', 'primaryTitle', 'originalTitle', 'startYear', 'director', 'movie_writing', 'writer'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "X_test = X_test.drop(['movie_train'], axis=1)\n",
    "\n",
    "# Replace '\\\\N' with NaN in the test set\n",
    "X_test.replace('\\\\N', np.nan, inplace=True)\n",
    "\n",
    "# Handle missing values in the test set\n",
    "for column in X_test.columns[X_test.isnull().any()]:\n",
    "    if X_test[column].dtype == 'float64':\n",
    "        X_test[column].fillna(X_test[column].mean(), inplace=True)\n",
    "    else:\n",
    "        X_test[column].fillna(X_test[column].mode()[0], inplace=True)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "submission = pd.DataFrame({'tconst': test_data['tconst'], 'label': y_pred_test})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54dd55f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
